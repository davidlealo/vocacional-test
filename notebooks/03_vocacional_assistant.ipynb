{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e732b703",
   "metadata": {},
   "source": [
    "# Asistente Vocacional FPUC â€” Colab\n",
    "\n",
    "Este notebook implementa un **asistente vocacional** usando **Azure OpenAI** con:\n",
    "- Embeddings (`emb-vocatest`) para bÃºsqueda semÃ¡ntica sobre *Binder1.pdf*\n",
    "- Chat (`phi4mini-chat`) para respuestas contextuales\n",
    "- Variables seguras desde `.env` con *fallback* a `getpass()`\n",
    "\n",
    "> **Requisitos previos**  \n",
    "> 1) Tener un recurso Azure OpenAI con deployments: `phi4mini-chat` y `emb-vocatest`  \n",
    "> 2) Subir tu archivo `Binder1.pdf` a la carpeta `data/` de este entorno  \n",
    "> 3) (Opcional) Subir `.env` en la raÃ­z del proyecto con tus credenciales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â¬‡ï¸ InstalaciÃ³n de dependencias\n",
    "!pip -q install pdfplumber python-dotenv requests tqdm scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b527d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Carga de variables de entorno desde .env y fallback interactivo\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except Exception as e:\n",
    "    print(\"python-dotenv no disponible, se usarÃ¡ solo input interactivo.\")\n",
    "\n",
    "if not os.getenv(\"AZURE_OPENAI_KEY\"):\n",
    "    os.environ[\"AZURE_OPENAI_KEY\"] = getpass(\"Introduce AZURE_OPENAI_KEY: \")\n",
    "\n",
    "if not os.getenv(\"AZURE_OPENAI_ENDPOINT\"):\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = input(\"Introduce AZURE_OPENAI_ENDPOINT (ej: https://oai-vocatest.services.ai.azure.com/): \").strip()\n",
    "\n",
    "if not os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"):\n",
    "    os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = input(\"Introduce AZURE_OPENAI_DEPLOYMENT (ej: phi4mini-chat): \").strip()\n",
    "\n",
    "if not os.getenv(\"AZURE_OPENAI_EMBEDDING\"):\n",
    "    os.environ[\"AZURE_OPENAI_EMBEDDING\"] = input(\"Introduce AZURE_OPENAI_EMBEDDING (ej: emb-vocatest): \").strip()\n",
    "\n",
    "print(\"âœ… Variables cargadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e79ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š Importaciones y helpers\n",
    "import pdfplumber\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "API_KEY = os.environ[\"AZURE_OPENAI_KEY\"]\n",
    "ENDPOINT = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "CHAT_DEPLOYMENT = os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n",
    "EMB_DEPLOYMENT  = os.environ[\"AZURE_OPENAI_EMBEDDING\"]\n",
    "\n",
    "CHAT_URL = f\"{ENDPOINT}openai/deployments/{CHAT_DEPLOYMENT}/chat/completions?api-version=2024-05-01-preview\"\n",
    "EMB_URL  = f\"{ENDPOINT}openai/deployments/{EMB_DEPLOYMENT}/embeddings?api-version=2024-05-01-preview\"\n",
    "\n",
    "HEADERS = {\"Content-Type\": \"application/json\", \"api-key\": API_KEY}\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    t = re.sub(r\"[\\u0000-\\u001F]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t.strip()\n",
    "\n",
    "def chunk_text(text: str, max_chars: int = 5000, overlap: int = 500):\n",
    "    # max_chars ~ aprox 800-1000 tokens segÃºn idioma; ajustable\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        chunk = text[i:i+max_chars]\n",
    "        chunks.append(chunk.strip())\n",
    "        i += max_chars - overlap\n",
    "    return [c for c in chunks if c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“„ Extraer texto desde Binder1.pdf (sube tu archivo a ./data/Binder1.pdf)\n",
    "from pathlib import Path\n",
    "\n",
    "pdf_path = Path(\"data/Binder1.pdf\")\n",
    "assert pdf_path.exists(), \"âŒ No se encontrÃ³ data/Binder1.pdf. Sube el archivo a la carpeta data/.\"\n",
    "\n",
    "pages_text = []\n",
    "with pdf_path.open(\"rb\") as f:\n",
    "    with pdfplumber.open(f) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            txt = page.extract_text() or \"\"\n",
    "            pages_text.append(txt)\n",
    "\n",
    "full_text = clean_text(\"\\n\".join(pages_text))\n",
    "print(\"âœ… Texto extraÃ­do. Longitud de caracteres:\", len(full_text))\n",
    "\n",
    "# Dividir en chunks\n",
    "chunks = chunk_text(full_text, max_chars=5000, overlap=500)\n",
    "print(f\"âœ… Chunks generados: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91761df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Generar embeddings de todos los chunks (puede tardar unos minutos segÃºn largo)\n",
    "def get_embeddings(text_list):\n",
    "    resp = requests.post(EMB_URL, headers=HEADERS, data=json.dumps({\"input\": text_list}))\n",
    "    if resp.status_code != 200:\n",
    "        raise RuntimeError(f\"Error embeddings: {resp.status_code} {resp.text}\")\n",
    "    data = resp.json()[\"data\"]\n",
    "    return [d[\"embedding\"] for d in data]\n",
    "\n",
    "# Procesar por lotes para evitar payloads grandes\n",
    "batch_size = 16\n",
    "all_embeddings = []\n",
    "for i in tqdm(range(0, len(chunks), batch_size), desc=\"Embedding batches\"):\n",
    "    batch = chunks[i:i+batch_size]\n",
    "    embs = get_embeddings(batch)\n",
    "    all_embeddings.extend(embs)\n",
    "\n",
    "emb_arr = np.array(all_embeddings, dtype=np.float32)\n",
    "print(\"âœ… Matriz de embeddings:\", emb_arr.shape)\n",
    "\n",
    "# Guardar a disco\n",
    "out_df = pd.DataFrame({\"chunk\": chunks, \"embedding\": list(all_embeddings)})\n",
    "out_df.to_pickle(\"binder_embeddings.pkl\")\n",
    "print(\"ğŸ’¾ Guardado: binder_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e611b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” BÃºsqueda semÃ¡ntica (top-k)\n",
    "def retrieve_context(query: str, top_k: int = 3):\n",
    "    r = requests.post(EMB_URL, headers=HEADERS, data=json.dumps({\"input\": query}))\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"Error embedding query: {r.status_code} {r.text}\")\n",
    "    q_emb = np.array(r.json()[\"data\"][0][\"embedding\"], dtype=np.float32).reshape(1, -1)\n",
    "    db = pd.read_pickle(\"binder_embeddings.pkl\")\n",
    "    M = np.vstack(db[\"embedding\"].values)\n",
    "    sims = cosine_similarity(q_emb, M)[0]\n",
    "    idxs = sims.argsort()[::-1][:top_k]\n",
    "    hits = db.iloc[idxs]\n",
    "    # devolver texto concatenado\n",
    "    context = \"\\n\\n---\\n\\n\".join(hits[\"chunk\"].tolist())\n",
    "    return context, hits[[\"chunk\"]]\n",
    "\n",
    "# PequeÃ±a prueba de recuperaciÃ³n\n",
    "ctx, refs = retrieve_context(\"Â¿QuÃ© es la gratuidad y cÃ³mo postular?\")\n",
    "print(ctx[:500], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§¾ Prompt institucional (puedes editarlo o cargarlo desde prompt_vocacional.txt)\n",
    "PROMPT_SISTEMA = \"\"\"Este asistente estÃ¡ diseÃ±ado para ayudar a postulantes a carreras universitarias o tÃ©cnicas en Chile a tomar decisiones informadas sobre su futuro acadÃ©mico y profesional. Trabaja en colaboraciÃ³n con la FundaciÃ³n Por Una Carrera y la ONG Innovacien, por lo que su enfoque es inclusivo, confiable, claro y siempre centrado en el bienestar del estudiante.\n",
    "\n",
    "Proporciona orientaciÃ³n sobre procesos de admisiÃ³n, requisitos de postulaciÃ³n, becas y beneficios estatales, opciones acadÃ©micas, vida universitaria y decisiones vocacionales.\n",
    "\n",
    "El asistente incorpora preguntas frecuentes y actividades de autoconocimiento extraÃ­das del material educativo de FundaciÃ³n Por Una Carrera (Binder1.pdf) y recursos oficiales del DEMRE/FUAS. Debe citar o recomendar recursos cuando corresponda.\n",
    "\n",
    "El asistente debe:\n",
    "- Explicar con claridad y sin tecnicismos innecesarios.\n",
    "- Adaptar sus respuestas a la realidad chilena actual.\n",
    "- Mostrar sensibilidad frente a contextos de vulnerabilidad social y educativa.\n",
    "- Proporcionar enlaces, fuentes oficiales y recursos cuando estÃ©n disponibles.\n",
    "- Promover la toma de decisiones informadas sin imponer opciones.\n",
    "\n",
    "Debe evitar:\n",
    "- Reemplazar la orientaciÃ³n profesional presencial.\n",
    "- Dar consejos cerrados o categÃ³ricos que limiten opciones.\n",
    "- Suponer informaciÃ³n sin fundamentos o sin contexto.\n",
    "\n",
    "Al comenzar, saluda indicando que esta es una iniciativa conjunta entre la FundaciÃ³n Por Una Carrera (https://www.instagram.com/porunacarrera) e Innovacien (https://www.instagram.com/innovacien), e invita a seguir ambas cuentas en Instagram y a compartir su experiencia con el chat.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d416a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¬ FunciÃ³n de respuesta con contexto\n",
    "def chat_with_context(user_query: str, top_k: int = 3, temperature: float = 0.4, max_tokens: int = 600):\n",
    "    context, refs = retrieve_context(user_query, top_k=top_k)\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": PROMPT_SISTEMA},\n",
    "            {\"role\": \"user\", \"content\": f\"Contexto relevante:\\n{context}\\n\\nPregunta del usuario: {user_query}\"},\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    resp = requests.post(CHAT_URL, headers=HEADERS, data=json.dumps(payload))\n",
    "    if resp.status_code != 200:\n",
    "        raise RuntimeError(f\"Error chat: {resp.status_code} {resp.text}\")\n",
    "    answer = resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return answer, refs\n",
    "\n",
    "# Prueba rÃ¡pida\n",
    "ans, refs = chat_with_context(\"Â¿CÃ³mo postulo a beneficios estudiantiles del Estado?\")\n",
    "print(\"ğŸ¤– Respuesta:\\n\", ans[:800], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a67e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§‘â€ğŸ’» Interfaz simple de conversaciÃ³n (ejecuta esta celda y luego llama a chat_with_context)\n",
    "q = \"Quiero estudiar pedagogÃ­a pero no sÃ© si califico para gratuidad. Â¿QuÃ© debo revisar?\"\n",
    "ans, refs = chat_with_context(q, top_k=3)\n",
    "print(\"ğŸ§ Usuario:\", q)\n",
    "print(\"\\nğŸ¤– Asistente:\\n\", ans)\n",
    "print(\"\\nğŸ” Fragmentos usados (primeros 200 chars de cada uno):\")\n",
    "for i, ch in enumerate(refs[\"chunk\"].tolist(), 1):\n",
    "    print(f\"[{i}] {ch[:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
