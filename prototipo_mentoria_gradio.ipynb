{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPSwn+xrsZc8t7XvDGUpLh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/vocacional-test/blob/main/prototipo_mentoria_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d81I5Dz1v-2v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9eSiU_IC0dMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HSsXJGSBvtU6",
        "outputId": "30f18b9a-4d5f-4d07-dd1c-378c5e1bfb10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.121.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.4)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4e700b124994b231d9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4e700b124994b231d9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Instalación de Gradio en Colab (ejecuta esto una vez)\n",
        "!pip install gradio\n",
        "\n",
        "import gradio as gr\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "# Simulaciones de funciones auxiliares (backend processes)\n",
        "def clasificar_tema(texto):\n",
        "    # Simulación: Clasifica el tema basado en palabras clave o random para demo\n",
        "    temas_posibles = [\"Biología\", \"Matemáticas\", \"Lenguaje\", \"Historia\", \"Orientación Vocacional\", \"Educación Superior\"]\n",
        "    return random.choice(temas_posibles)\n",
        "\n",
        "def detectar_emocion(texto):\n",
        "    # Simulación: Detecta emoción basada en palabras o random para demo\n",
        "    emociones = [\"Entusiasmo\", \"Confusión\", \"Curiosidad\", \"Frustración\", \"Motivación\", \"Ansiedad\"]\n",
        "    return random.choice(emociones)\n",
        "\n",
        "def estimar_comprension(texto):\n",
        "    # Simulación: Estima nivel de comprensión basado en longitud o random para demo\n",
        "    niveles = [\"Alto (80-100%)\", \"Medio (60-79%)\", \"Bajo (<60%)\"]\n",
        "    return random.choice(niveles)\n",
        "\n",
        "def generar_ticket_salida(texto):\n",
        "    # Simulación: Genera preguntas de ticket de salida relevantes al tema\n",
        "    preguntas = [\n",
        "        \"1. ¿Qué gas absorben las plantas durante la fotosíntesis?\\nA) Oxígeno\\nB) Nitrógeno\\nC) Dióxido de carbono\\nD) Hidrógeno\",\n",
        "        \"2. ¿Qué parte de la célula contiene el material genético?\\nA) Mitocondria\\nB) Núcleo\\nC) Ribosoma\\nD) Membrana\",\n",
        "        \"3. Explica con tus palabras por qué la luz solar es importante para las plantas.\",\n",
        "        \"4. ¿Qué fue lo más interesante que aprendiste hoy y por qué?\",\n",
        "        \"5. ¿Qué carrera te interesa y por qué? (Basado en la orientación vocacional discutida)\"\n",
        "    ]\n",
        "    return \"\\n\\n\".join(random.sample(preguntas, 3))  # Selecciona 3 preguntas al azar para variedad\n",
        "\n",
        "# Simulación de almacenamiento (opcional, pero muestra logs)\n",
        "registro = []\n",
        "def guardar_conversacion(texto, tema, emocion, comprension, ticket):\n",
        "    fecha = datetime.datetime.now().isoformat()\n",
        "    registro.append({\n",
        "        \"fecha\": fecha,\n",
        "        \"tema\": tema,\n",
        "        \"emocion\": emocion,\n",
        "        \"comprension\": comprension,\n",
        "        \"ticket\": ticket,\n",
        "        \"texto\": texto\n",
        "    })\n",
        "    # Retorna un log para mostrar en front\n",
        "    return f\"[LOG] Conversación guardada en registro (ID: {len(registro)}). Fecha: {fecha}\"\n",
        "\n",
        "# Función principal: Procesa la entrada y genera salidas (backend logic)\n",
        "def analizar_conversacion(texto):\n",
        "    # Paso 1: Clasificar tema\n",
        "    tema = clasificar_tema(texto)\n",
        "    log = f\"[LOG] Paso 1: Tema clasificado como '{tema}' basado en el texto proporcionado.\\n\"\n",
        "\n",
        "    # Paso 2: Detectar emoción\n",
        "    emocion = detectar_emocion(texto)\n",
        "    log += f\"[LOG] Paso 2: Emoción detectada como '{emocion}' analizando el tono del texto.\\n\"\n",
        "\n",
        "    # Paso 3: Estimar comprensión\n",
        "    comprension = estimar_comprension(texto)\n",
        "    log += f\"[LOG] Paso 3: Nivel de comprensión estimado en '{comprension}' evaluando la complejidad del texto.\\n\"\n",
        "\n",
        "    # Paso 4: Generar ticket de salida\n",
        "    ticket = generar_ticket_salida(texto)\n",
        "    log += f\"[LOG] Paso 4: Ticket de salida generado con preguntas relevantes.\\n\"\n",
        "\n",
        "    # Paso 5: Guardar conversación\n",
        "    save_log = guardar_conversacion(texto, tema, emocion, comprension, ticket)\n",
        "    log += save_log + \"\\n\"\n",
        "\n",
        "    # Log final\n",
        "    log += \"[LOG] Proceso completado exitosamente.\"\n",
        "\n",
        "    return tema, emocion, comprension, ticket, log\n",
        "\n",
        "# Interfaz Gradio (frontend)\n",
        "iface = gr.Interface(\n",
        "    fn=analizar_conversacion,  # Función backend\n",
        "    inputs=gr.Textbox(lines=15, label=\"Transcripción de la conversación\"),  # Input del usuario\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Tema identificado\"),  # Output 1\n",
        "        gr.Textbox(label=\"Emoción detectada\"),  # Output 2\n",
        "        gr.Textbox(label=\"Nivel de comprensión\"),  # Output 3\n",
        "        gr.Textbox(lines=10, label=\"Ticket de salida generado\"),  # Output 4\n",
        "        gr.Textbox(lines=5, label=\"Logs del sistema (Proceso backend)\")  # Output 5: Muestra los logs del proceso backend en el front\n",
        "    ],\n",
        "    title=\"Analizador de Conversaciones de Mentoría\",\n",
        "    description=\"Ingresa la transcripción completa de una sesión de mentoría. El sistema clasificará la conversación, detectará emociones, estimará comprensión y generará un ticket de salida. Los logs muestran el proceso backend paso a paso.\"\n",
        ")\n",
        "\n",
        "# Lanzar la app en Colab (con share=True para enlace público)\n",
        "iface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de dependencias en Colab\n",
        "!pip install gradio python-dotenv openai requests\n",
        "\n",
        "import gradio as gr\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import AzureOpenAI\n",
        "import requests\n",
        "import datetime\n",
        "\n",
        "# === Configuración de Azure (Search y OpenAI) ===\n",
        "load_dotenv()  # Carga variables de .env (sube tu archivo .env a Colab)\n",
        "\n",
        "# Validar variables requeridas\n",
        "required_vars = [\n",
        "    \"AZURE_SEARCH_API_KEY\", \"AZURE_SEARCH_ENDPOINT\", \"AZURE_SEARCH_INDEX_NAME\",\n",
        "    \"AZURE_OPENAI_API_KEY\", \"AZURE_OPENAI_ENDPOINT\", \"AZURE_OPENAI_DEPLOYMENT_NAME\"\n",
        "]\n",
        "for var in required_vars:\n",
        "    if not os.environ.get(var):\n",
        "        raise ValueError(f\"Falta variable de entorno: {var}\")\n",
        "\n",
        "# Configuración de Azure Search\n",
        "AZURE_SEARCH_API_KEY = os.environ[\"AZURE_SEARCH_API_KEY\"]\n",
        "AZURE_SEARCH_ENDPOINT = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
        "AZURE_SEARCH_INDEX_NAME = os.environ[\"AZURE_SEARCH_INDEX_NAME\"]\n",
        "\n",
        "# Inicializar cliente de Azure OpenAI\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    api_version=\"2023-08-01-preview\",\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
        ")\n",
        "\n",
        "# === Función de Búsqueda en Azure Search (RAG Retrieval) ===\n",
        "def search_documents(query: str, top_k: int = 5) -> str:\n",
        "    \"\"\"Realiza una búsqueda en Azure Cognitive Search y retorna el contexto concatenado.\"\"\"\n",
        "    try:\n",
        "        url = f\"{AZURE_SEARCH_ENDPOINT}/indexes/{AZURE_SEARCH_INDEX_NAME}/docs/search?api-version=2023-07-01-Preview\"\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"api-key\": AZURE_SEARCH_API_KEY\n",
        "        }\n",
        "        payload = {\n",
        "            \"search\": query,\n",
        "            \"top\": top_k\n",
        "        }\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        results = response.json()\n",
        "        documents = [doc.get(\"content\", \"\") for doc in results.get(\"value\", [])]\n",
        "        return \"\\n\\n\".join(documents) if documents else \"No se encontraron documentos relevantes.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error en búsqueda: {str(e)}\"\n",
        "\n",
        "# === Función de Generación de Respuesta (RAG Generation) ===\n",
        "def generate_answer(question: str, context: str) -> str:\n",
        "    \"\"\"Genera una respuesta usando Azure OpenAI basada en pregunta y contexto de búsqueda.\"\"\"\n",
        "    if not context.strip():\n",
        "        return \"No hay contexto suficiente para responder. Intenta reformular la pregunta.\"\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    Eres un mentor pedagógico especializado en orientación vocacional y educación.\n",
        "    Responde de forma clara, breve y útil, basándote únicamente en el contexto proporcionado.\n",
        "    Si el contexto es insuficiente, indica que no puedes responder con precisión.\n",
        "    \"\"\"\n",
        "    user_prompt = f\"Pregunta: {question}\\n\\nContexto:\\n{context}\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            max_tokens=500\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error al generar respuesta: {str(e)}\"\n",
        "\n",
        "# === Backend: Funciones de Clasificación con Azure OpenAI ===\n",
        "def clasificar_tema(transcripcion: str) -> str:\n",
        "    \"\"\"Clasifica el tema usando Azure OpenAI.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Clasifica el tema principal de esta transcripción de una sesión de mentoría en una de estas categorías:\n",
        "    Biología, Matemáticas, Lenguaje, Historia, Orientación Vocacional, Educación Superior, Otros.\n",
        "    Responde solo con el nombre de la categoría.\n",
        "\n",
        "    Transcripción: {transcripcion}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def detectar_emocion(transcripcion: str) -> str:\n",
        "    \"\"\"Detecta la emoción usando Azure OpenAI.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Detecta la emoción predominante en esta transcripción de mentoría (Entusiasmo, Confusión, Curiosidad, Frustración, Motivación, Ansiedad).\n",
        "    Responde solo con el nombre de la emoción.\n",
        "\n",
        "    Transcripción: {transcripcion}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def estimar_comprension(transcripcion: str) -> str:\n",
        "    \"\"\"Estima el nivel de comprensión usando Azure OpenAI.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Estima el nivel de comprensión del estudiante en esta transcripción (Alto (80-100%), Medio (60-79%), Bajo (<60%)).\n",
        "    Responde solo con el nivel.\n",
        "\n",
        "    Transcripción: {transcripcion}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def generar_ticket_salida(transcripcion: str, tema: str) -> str:\n",
        "    \"\"\"Genera un ticket de salida usando Azure OpenAI, adaptado al tema.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Genera 3 preguntas de ticket de salida relevantes para reforzar el aprendizaje en una sesión de mentoría sobre '{tema}'.\n",
        "    Incluye una mezcla de preguntas de opción múltiple y abiertas.\n",
        "    Formato: Pregunta 1\\n[opciones si aplica]\\n\\nPregunta 2\\n...\\n\\nPregunta 3\\n...\n",
        "\n",
        "    Basado en esta transcripción: {transcripcion[:1000]}  # Limitar longitud\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def guardar_conversacion(transcripcion: str, tema: str, emocion: str, comprension: str, ticket: str) -> str:\n",
        "    \"\"\"Guarda en registro (simulado) y retorna log.\"\"\"\n",
        "    fecha = datetime.datetime.now().isoformat()\n",
        "    registro = {\n",
        "        \"fecha\": fecha,\n",
        "        \"tema\": tema,\n",
        "        \"emocion\": emocion,\n",
        "        \"comprension\": comprension,\n",
        "        \"ticket\": ticket,\n",
        "        \"transcripcion\": transcripcion\n",
        "    }\n",
        "    global registros\n",
        "    registros.append(registro)\n",
        "    return f\"[LOG] Conversación guardada (ID: {len(registros)}). Fecha: {fecha}\"\n",
        "\n",
        "# === Backend: Función Principal ===\n",
        "def procesar_pregunta(pregunta: str):\n",
        "    \"\"\"Procesa la pregunta con RAG y luego clasifica la transcripción (pregunta + respuesta).\"\"\"\n",
        "    if not pregunta.strip():\n",
        "        return \"Error: Ingresa una pregunta válida.\", \"\", \"\", \"\", \"\", \"[LOG] Error: Pregunta vacía.\"\n",
        "\n",
        "    log = \"[LOG] Iniciando procesamiento de pregunta...\\n\"\n",
        "\n",
        "    # Paso 1: Búsqueda en Azure Search (Retrieval)\n",
        "    contexto = search_documents(pregunta)\n",
        "    log += f\"[LOG] Paso 1: Búsqueda completada. Contexto recuperado: {len(contexto)} caracteres.\\n\"\n",
        "\n",
        "    # Paso 2: Generar respuesta con RAG (Generation)\n",
        "    respuesta = generate_answer(pregunta, contexto)\n",
        "    log += \"[LOG] Paso 2: Respuesta generada vía Azure OpenAI.\\n\"\n",
        "\n",
        "    # Crear transcripción para clasificación: pregunta + respuesta\n",
        "    transcripcion = f\"Pregunta del usuario: {pregunta}\\nRespuesta del mentor: {respuesta}\"\n",
        "\n",
        "    # Paso 3: Clasificar tema\n",
        "    tema = clasificar_tema(transcripcion)\n",
        "    log += f\"[LOG] Paso 3: Tema clasificado como '{tema}' vía Azure OpenAI.\\n\"\n",
        "\n",
        "    # Paso 4: Detectar emoción\n",
        "    emocion = detectar_emocion(transcripcion)\n",
        "    log += f\"[LOG] Paso 4: Emoción detectada como '{emocion}' vía Azure OpenAI.\\n\"\n",
        "\n",
        "    # Paso 5: Estimar comprensión\n",
        "    comprension = estimar_comprension(transcripcion)\n",
        "    log += f\"[LOG] Paso 5: Nivel de comprensión estimado en '{comprension}' vía Azure OpenAI.\\n\"\n",
        "\n",
        "    # Paso 6: Generar ticket de salida\n",
        "    ticket = generar_ticket_salida(transcripcion, tema)\n",
        "    log += f\"[LOG] Paso 6: Ticket de salida generado para '{tema}'.\\n\"\n",
        "\n",
        "    # Paso 7: Guardar conversación\n",
        "    save_log = guardar_conversacion(transcripcion, tema, emocion, comprension, ticket)\n",
        "    log += save_log + \"\\n\"\n",
        "\n",
        "    log += \"[LOG] Procesamiento completado exitosamente con Azure.\"\n",
        "\n",
        "    return respuesta, tema, emocion, comprension, ticket, log\n",
        "\n",
        "# === Inicialización del Registro ===\n",
        "registros = []\n",
        "\n",
        "# === Frontend: Interfaz Gradio ===\n",
        "iface = gr.Interface(\n",
        "    fn=procesar_pregunta,\n",
        "    inputs=gr.Textbox(lines=5, label=\"Ingresa tu pregunta sobre orientación vocacional\", placeholder=\"Ejemplo: ¿Qué es Proyectate?\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(lines=10, label=\"Respuesta del RAG (Azure Search + OpenAI)\"),\n",
        "        gr.Textbox(label=\"Tema identificado (Clasificación)\"),\n",
        "        gr.Textbox(label=\"Emoción detectada (Clasificación)\"),\n",
        "        gr.Textbox(label=\"Nivel de comprensión (Clasificación)\"),\n",
        "        gr.Textbox(lines=10, label=\"Ticket de salida generado (Clasificación)\"),\n",
        "        gr.Textbox(lines=5, label=\"Logs del sistema (Backend con Azure)\")\n",
        "    ],\n",
        "    title=\"Chat Vocacional con RAG y Clasificación (Azure)\",\n",
        "    description=\"Ingresa una pregunta para obtener una respuesta del sistema RAG (búsqueda en Azure Search + generación con Azure OpenAI). Luego, el sistema clasificará la conversación (pregunta + respuesta) en tema, emoción, comprensión y generará un ticket de salida. Los logs muestran el proceso backend paso a paso.\"\n",
        ")\n",
        "\n",
        "# Lanzar la app en Colab\n",
        "iface.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5pWMalt01eGD",
        "outputId": "324f1db9-f7f9-4f88-9d20-125df6bc61ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.121.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.4)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9e5a8cbd85138e9873.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9e5a8cbd85138e9873.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://4e700b124994b231d9.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://9e5a8cbd85138e9873.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```markdown\n",
        "# Código Markdown: Chat Vocacional con RAG Semántico y Clasificación (Azure)\n",
        "\n",
        "## Instalación de Dependencias en Colab\n",
        "```bash\n",
        "!pip install gradio python-dotenv openai requests\n",
        "```\n",
        "\n",
        "## Importación de Módulos\n",
        "```python\n",
        "import gradio as gr\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import AzureOpenAI\n",
        "import requests\n",
        "import datetime\n",
        "import json  # Para depuración de respuestas JSON\n",
        "```\n",
        "\n",
        "## Configuración de Azure (Search y OpenAI)\n",
        "```python\n",
        "# Carga variables de .env (sube tu archivo .env a Colab)\n",
        "load_dotenv()\n",
        "\n",
        "# Validar variables requeridas\n",
        "required_vars = [\n",
        "    \"AZURE_SEARCH_API_KEY\", \"AZURE_SEARCH_ENDPOINT\", \"AZURE_SEARCH_INDEX_NAME\",\n",
        "    \"AZURE_OPENAI_API_KEY\", \"AZURE_OPENAI_ENDPOINT\", \"AZURE_OPENAI_DEPLOYMENT_NAME\"\n",
        "]\n",
        "for var in required_vars:\n",
        "    if not os.environ.get(var):\n",
        "        raise ValueError(f\"Falta variable de entorno: {var}\")\n",
        "\n",
        "# Configuración de Azure Search\n",
        "AZURE_SEARCH_API_KEY = os.environ[\"AZURE_SEARCH_API_KEY\"]\n",
        "AZURE_SEARCH_ENDPOINT = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
        "AZURE_SEARCH_INDEX_NAME = os.environ[\"AZURE_SEARCH_INDEX_NAME\"]\n",
        "\n",
        "# Inicializar cliente de Azure OpenAI\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    api_version=\"2023-08-01-preview\",\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
        ")\n",
        "```\n",
        "\n",
        "## Función de Búsqueda en Azure Search (RAG Retrieval) - Mejorada con Semántica y Depuración\n",
        "```python\n",
        "def search_documents(query: str, top_k: int = 5) -> str:\n",
        "    \"\"\"Realiza una búsqueda semántica en Azure Cognitive Search y retorna el contexto concatenado.\"\"\"\n",
        "    try:\n",
        "        url = f\"{AZURE_SEARCH_ENDPOINT}/indexes/{AZURE_SEARCH_INDEX_NAME}/docs/search?api-version=2023-07-01-Preview\"\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"api-key\": AZURE_SEARCH_API_KEY\n",
        "        }\n",
        "        payload = {\n",
        "            \"search\": query,\n",
        "            \"top\": top_k,\n",
        "            \"queryType\": \"semantic\",  # Habilitar búsqueda semántica para mejores resultados\n",
        "            \"semanticConfiguration\": \"default\",  # Asumir config semántica 'default'; ajusta si es diferente\n",
        "            \"select\": \"content,source,@search.score\"  # Seleccionar campos relevantes\n",
        "        }\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        results = response.json()\n",
        "        \n",
        "        # Depuración: Imprimir resultados crudos para logs (opcional, remueve en prod)\n",
        "        print(f\"[DEBUG] Respuesta de Azure Search: {json.dumps(results, indent=2)[:500]}...\")  # Primeros 500 chars\n",
        "        \n",
        "        documents = [doc.get(\"content\", \"\") for doc in results.get(\"value\", [])]\n",
        "        contexto = \"\\n\\n\".join(documents)\n",
        "        if not contexto.strip():\n",
        "            return \"No se encontraron documentos relevantes. Verifica el índice o reformula la consulta.\"\n",
        "        return contexto\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        error_msg = f\"Error HTTP en búsqueda: {e.response.status_code} - {e.response.text[:200]}\"\n",
        "        print(f\"[ERROR] {error_msg}\")\n",
        "        return f\"Error en búsqueda: {error_msg}\"\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error inesperado en búsqueda: {str(e)}\"\n",
        "        print(f\"[ERROR] {error_msg}\")\n",
        "        return f\"Error en búsqueda: {error_msg}\"\n",
        "```\n",
        "\n",
        "## Función de Generación de Respuesta (RAG Generation)\n",
        "```python\n",
        "def generate_answer(question: str, context: str) -> str:\n",
        "    \"\"\"Genera una respuesta usando Azure OpenAI basada en pregunta y contexto de búsqueda.\"\"\"\n",
        "    if not context or \"No se encontraron\" in context:\n",
        "        return \"No hay contexto suficiente para responder con precisión. Intenta reformular la pregunta o verifica que el índice de Azure Search tenga datos relevantes sobre orientación vocacional.\"\n",
        "    system_prompt = \"\"\"\n",
        "    Eres un mentor pedagógico especializado en orientación vocacional y educación en Chile.\n",
        "    Responde de forma clara, breve, útil y empática, basándote únicamente en el contexto proporcionado.\n",
        "    Si el contexto es insuficiente o no contiene información relevante, indica que no puedes responder con precisión y sugiere al usuario proporcionar más detalles o contactar instituciones oficiales como el Mineduc.\n",
        "    Estructura la respuesta con pasos accionables si aplica.\n",
        "    \"\"\"\n",
        "    user_prompt = f\"Pregunta: {question}\\n\\nContexto:\\n{context[:2000]}\"  # Limitar contexto para tokens\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            max_tokens=500\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error al generar respuesta: {str(e)}\"\n",
        "```\n",
        "\n",
        "## Backend: Funciones de Clasificación con Azure OpenAI\n",
        "### Clasificar Tema\n",
        "```python\n",
        "def clasificar_tema(transcripcion: str) -> str:\n",
        "    \"\"\"Clasifica el tema usando Azure OpenAI.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Clasifica el tema principal de esta transcripción de una sesión de mentoría en una de estas categorías:\n",
        "    Biología, Matemáticas, Lenguaje, Historia, Orientación Vocacional, Educación Superior, Otros.\n",
        "    Responde solo con el nombre de la categoría.\n",
        "   \n",
        "    Transcripción: {transcripcion[:1000]}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "```\n",
        "\n",
        "### Detectar Emoción\n",
        "```python\n",
        "def detectar_emocion(transcripcion: str) -> str:\n",
        "    \"\"\"Detecta la emoción usando Azure OpenAI.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Detecta la emoción predominante en esta transcripción de mentoría (Entusiasmo, Confusión, Curiosidad, Frustración, Motivación, Ansiedad).\n",
        "    Responde solo con el nombre de la emoción.\n",
        "   \n",
        "    Transcripción: {transcripcion[:1000]}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "```\n",
        "\n",
        "### Estimar Comprensión\n",
        "```python\n",
        "def estimar_comprension(transcripcion: str) -> str:\n",
        "    \"\"\"Estima el nivel de comprensión usando Azure OpenAI.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Estima el nivel de comprensión del estudiante en esta transcripción (Alto (80-100%), Medio (60-79%), Bajo (<60%)).\n",
        "    Responde solo con el nivel.\n",
        "   \n",
        "    Transcripción: {transcripcion[:1000]}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "```\n",
        "\n",
        "### Generar Ticket de Salida\n",
        "```python\n",
        "def generar_ticket_salida(transcripcion: str, tema: str) -> str:\n",
        "    \"\"\"Genera un ticket de salida usando Azure OpenAI, adaptado al tema.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Genera 3 preguntas de ticket de salida relevantes para reforzar el aprendizaje en una sesión de mentoría sobre '{tema}' en contexto chileno (e.g., Mineduc, crédito universitario).\n",
        "    Incluye una mezcla de preguntas de opción múltiple y abiertas.\n",
        "    Formato: Pregunta 1\\n[opciones si aplica]\\n\\nPregunta 2\\n...\\n\\nPregunta 3\\n...\n",
        "   \n",
        "    Basado en esta transcripción: {transcripcion[:1000]}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "```\n",
        "\n",
        "### Guardar Conversación\n",
        "```python\n",
        "def guardar_conversacion(transcripcion: str, tema: str, emocion: str, comprension: str, ticket: str) -> str:\n",
        "    \"\"\"Guarda en registro (simulado) y retorna log.\"\"\"\n",
        "    fecha = datetime.datetime.now().isoformat()\n",
        "    registro = {\n",
        "        \"fecha\": fecha,\n",
        "        \"tema\": tema,\n",
        "        \"emocion\": emocion,\n",
        "        \"comprension\": comprension,\n",
        "        \"ticket\": ticket,\n",
        "        \"transcripcion\": transcripcion\n",
        "    }\n",
        "    global registros\n",
        "    registros.append(registro)\n",
        "    return f\"[LOG] Conversación guardada (ID: {len(registros)}). Fecha: {fecha}\"\n",
        "```\n",
        "\n",
        "## Backend: Función Principal\n",
        "```python\n",
        "def procesar_pregunta(pregunta: str):\n",
        "    \"\"\"Procesa la pregunta con RAG (semántico) y luego clasifica la transcripción (pregunta + respuesta).\"\"\"\n",
        "    if not pregunta.strip():\n",
        "        return \"Error: Ingresa una pregunta válida.\", \"\", \"\", \"\", \"\", \"[LOG] Error: Pregunta vacía.\"\n",
        "    log = \"[LOG] Iniciando procesamiento de pregunta...\\n\"\n",
        "   \n",
        "    # Paso 1: Búsqueda semántica en Azure Search (Retrieval)\n",
        "    contexto = search_documents(pregunta)\n",
        "    log += f\"[LOG] Paso 1: Búsqueda semántica completada. Contexto recuperado: {len(contexto)} caracteres.\\n\"\n",
        "    if \"No se encontraron\" in contexto:\n",
        "        log += \"[LOG] ADVERTENCIA: Búsqueda sin resultados. Verifica el índice en Azure Portal (poblado con chunks de documentos vocacionales).\\n\"\n",
        "   \n",
        "    # Paso 2: Generar respuesta con RAG (Generation)\n",
        "    respuesta = generate_answer(pregunta, contexto)\n",
        "    log += \"[LOG] Paso 2: Respuesta generada vía Azure OpenAI.\\n\"\n",
        "   \n",
        "    # Crear transcripción para clasificación: pregunta + respuesta\n",
        "    transcripcion = f\"Pregunta del usuario: {pregunta}\\nRespuesta del mentor: {respuesta}\"\n",
        "   \n",
        "    # Paso 3: Clasificar tema\n",
        "    tema = clasificar_tema(transcripcion)\n",
        "    log += f\"[LOG] Paso 3: Tema clasificado como '{tema}' vía Azure OpenAI.\\n\"\n",
        "   \n",
        "    # Paso 4: Detectar emoción\n",
        "    emocion = detectar_emocion(transcripcion)\n",
        "    log += f\"[LOG] Paso 4: Emoción detectada como '{emocion}' vía Azure OpenAI.\\n\"\n",
        "   \n",
        "    # Paso 5: Estimar comprensión\n",
        "    comprension = estimar_comprension(transcripcion)\n",
        "    log += f\"[LOG] Paso 5: Nivel de comprensión estimado en '{comprension}' vía Azure OpenAI.\\n\"\n",
        "   \n",
        "    # Paso 6: Generar ticket de salida\n",
        "    ticket = generar_ticket_salida(transcripcion, tema)\n",
        "    log += f\"[LOG] Paso 6: Ticket de salida generado para '{tema}'.\\n\"\n",
        "   \n",
        "    # Paso 7: Guardar conversación\n",
        "    save_log = guardar_conversacion(transcripcion, tema, emocion, comprension, ticket)\n",
        "    log += save_log + \"\\n\"\n",
        "   \n",
        "    log += \"[LOG] Procesamiento completado exitosamente con Azure (búsqueda semántica activada).\"\n",
        "   \n",
        "    return respuesta, tema, emocion, comprension, ticket, log\n",
        "```\n",
        "\n",
        "## Inicialización del Registro\n",
        "```python\n",
        "registros = []\n",
        "```\n",
        "\n",
        "## Frontend: Interfaz Gradio\n",
        "```python\n",
        "iface = gr.Interface(\n",
        "    fn=procesar_pregunta,\n",
        "    inputs=gr.Textbox(lines=5, label=\"Ingresa tu pregunta sobre orientación vocacional\", placeholder=\"Ejemplo: ¿Qué debo hacer si perdí mis beneficios del crédito universitario?\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(lines=10, label=\"Respuesta del RAG (Búsqueda Semántica + OpenAI)\"),\n",
        "        gr.Textbox(label=\"Tema identificado (Clasificación)\"),\n",
        "        gr.Textbox(label=\"Emoción detectada (Clasificación)\"),\n",
        "        gr.Textbox(label=\"Nivel de comprensión (Clasificación)\"),\n",
        "        gr.Textbox(lines=10, label=\"Ticket de salida generado (Clasificación)\"),\n",
        "        gr.Textbox(lines=7, label=\"Logs del sistema (Backend con Azure - Incluye Debug)\")\n",
        "    ],\n",
        "    title=\"Chat Vocacional con RAG Semántico y Clasificación (Azure)\",\n",
        "    description=\"Ingresa una pregunta para obtener una respuesta mejorada del sistema RAG (búsqueda semántica en Azure Search + generación con Azure OpenAI). Luego, clasifica la conversación. Logs incluyen debug para depurar búsquedas vacías.\"\n",
        ")\n",
        "```\n",
        "\n",
        "## Lanzar la App en Colab\n",
        "```python\n",
        "iface.launch(share=True, debug=True)\n",
        "```\n",
        "```"
      ],
      "metadata": {
        "id": "i-RsXUkv3fcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de dependencias en Colab\n",
        "#!pip install gradio python-dotenv openai requests\n",
        "\n",
        "import gradio as gr\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import AzureOpenAI\n",
        "import requests\n",
        "import datetime\n",
        "import json  # Para depuración de respuestas JSON\n",
        "\n",
        "# === Configuración de Azure (Search y OpenAI) ===\n",
        "load_dotenv()  # Carga variables de .env (sube tu archivo .env a Colab)\n",
        "\n",
        "# Validar variables requeridas\n",
        "required_vars = [\n",
        "    \"AZURE_SEARCH_API_KEY\", \"AZURE_SEARCH_ENDPOINT\", \"AZURE_SEARCH_INDEX_NAME\",\n",
        "    \"AZURE_OPENAI_API_KEY\", \"AZURE_OPENAI_ENDPOINT\", \"AZURE_OPENAI_DEPLOYMENT_NAME\"\n",
        "]\n",
        "for var in required_vars:\n",
        "    if not os.environ.get(var):\n",
        "        raise ValueError(f\"Falta variable de entorno: {var}\")\n",
        "\n",
        "# Configuración de Azure Search\n",
        "AZURE_SEARCH_API_KEY = os.environ[\"AZURE_SEARCH_API_KEY\"]\n",
        "AZURE_SEARCH_ENDPOINT = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
        "AZURE_SEARCH_INDEX_NAME = os.environ[\"AZURE_SEARCH_INDEX_NAME\"]\n",
        "\n",
        "# Inicializar cliente de Azure OpenAI\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    api_version=\"2023-08-01-preview\",\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
        ")\n",
        "\n",
        "# === Función de Búsqueda en Azure Search (RAG Retrieval) - Mejorada con Semántica y Depuración ===\n",
        "def search_documents(query: str, top_k: int = 5) -> str:\n",
        "    \"\"\"Realiza una búsqueda semántica en Azure Cognitive Search y retorna el contexto concatenado.\"\"\"\n",
        "    try:\n",
        "        url = f\"{AZURE_SEARCH_ENDPOINT}/indexes/{AZURE_SEARCH_INDEX_NAME}/docs/search?api-version=2023-07-01-Preview\"\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"api-key\": AZURE_SEARCH_API_KEY\n",
        "        }\n",
        "        payload = {\n",
        "            \"search\": query,\n",
        "            \"top\": top_k,\n",
        "            \"queryType\": \"semantic\",  # Habilitar búsqueda semántica para mejores resultados\n",
        "            \"semanticConfiguration\": \"default\",  # Asumir config semántica 'default'; ajusta si es diferente\n",
        "            \"select\": \"content,source,@search.score\"  # Seleccionar campos relevantes\n",
        "        }\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        results = response.json()\n",
        "\n",
        "        # Depuración: Imprimir resultados crudos para logs (opcional, remueve en prod)\n",
        "        print(f\"[DEBUG] Respuesta de Azure Search: {json.dumps(results, indent=2)[:500]}...\")  # Primeros 500 chars\n",
        "\n",
        "        documents = [doc.get(\"content\", \"\") for doc in results.get(\"value\", [])]\n",
        "        contexto = \"\\n\\n\".join(documents)\n",
        "        if not contexto.strip():\n",
        "            return \"No se encontraron documentos relevantes. Verifica el índice o reformula la consulta.\"\n",
        "        return contexto\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        error_msg = f\"Error HTTP en búsqueda: {e.response.status_code} - {e.response.text[:200]}\"\n",
        "        print(f\"[ERROR] {error_msg}\")\n",
        "        return f\"Error en búsqueda: {error_msg}\"\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error inesperado en búsqueda: {str(e)}\"\n",
        "        print(f\"[ERROR] {error_msg}\")\n",
        "        return f\"Error en búsqueda: {error_msg}\"\n",
        "\n",
        "# === Función de Generación de Respuesta (RAG Generation) ===\n",
        "def generate_answer(question: str, context: str) -> str:\n",
        "    \"\"\"Genera una respuesta usando Azure OpenAI basada en pregunta y contexto de búsqueda.\"\"\"\n",
        "    if not context or \"No se encontraron\" in context:\n",
        "        return \"No hay contexto suficiente para responder con precisión. Intenta reformular la pregunta o verifica que el índice de Azure Search tenga datos relevantes sobre orientación vocacional.\"\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    Eres un mentor pedagógico especializado en orientación vocacional y educación en Chile.\n",
        "    Responde de forma clara, breve, útil y empática, basándote únicamente en el contexto proporcionado.\n",
        "    Si el contexto es insuficiente o no contiene información relevante, indica que no puedes responder con precisión y sugiere al usuario proporcionar más detalles o contactar instituciones oficiales como el Mineduc.\n",
        "    Estructura la respuesta con pasos accionables si aplica.\n",
        "    \"\"\"\n",
        "    user_prompt = f\"Pregunta: {question}\\n\\nContexto:\\n{context[:2000]}\"  # Limitar contexto para tokens\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            max_tokens=500\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error al generar respuesta: {str(e)}\"\n",
        "\n",
        "# === Backend: Funciones de Clasificación con Azure OpenAI ===\n",
        "def clasificar_tema(transcripcion: str) -> str:\n",
        "    \"\"\"Clasifica el tema usando Azure OpenAI.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Clasifica el tema principal de esta transcripción de una sesión de mentoría en una de estas categorías:\n",
        "    Biología, Matemáticas, Lenguaje, Historia, Orientación Vocacional, Educación Superior, Otros.\n",
        "    Responde solo con el nombre de la categoría.\n",
        "\n",
        "    Transcripción: {transcripcion[:1000]}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def detectar_emocion(transcripcion: str) -> str:\n",
        "    \"\"\"Detecta la emoción usando Azure OpenAI.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Detecta la emoción predominante en esta transcripción de mentoría (Entusiasmo, Confusión, Curiosidad, Frustración, Motivación, Ansiedad).\n",
        "    Responde solo con el nombre de la emoción.\n",
        "\n",
        "    Transcripción: {transcripcion[:1000]}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def estimar_comprension(transcripcion: str) -> str:\n",
        "    \"\"\"Estima el nivel de comprensión usando Azure OpenAI.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Estima el nivel de comprensión del estudiante en esta transcripción (Alto (80-100%), Medio (60-79%), Bajo (<60%)).\n",
        "    Responde solo con el nivel.\n",
        "\n",
        "    Transcripción: {transcripcion[:1000]}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def generar_ticket_salida(transcripcion: str, tema: str) -> str:\n",
        "    \"\"\"Genera un ticket de salida usando Azure OpenAI, adaptado al tema.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Genera 3 preguntas de ticket de salida relevantes para reforzar el aprendizaje en una sesión de mentoría sobre '{tema}' en contexto chileno (e.g., Mineduc, crédito universitario).\n",
        "    Incluye una mezcla de preguntas de opción múltiple y abiertas.\n",
        "    Formato: Pregunta 1\\n[opciones si aplica]\\n\\nPregunta 2\\n...\\n\\nPregunta 3\\n...\n",
        "\n",
        "    Basado en esta transcripción: {transcripcion[:1000]}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def guardar_conversacion(transcripcion: str, tema: str, emocion: str, comprension: str, ticket: str) -> str:\n",
        "    \"\"\"Guarda en registro (simulado) y retorna log.\"\"\"\n",
        "    fecha = datetime.datetime.now().isoformat()\n",
        "    registro = {\n",
        "        \"fecha\": fecha,\n",
        "        \"tema\": tema,\n",
        "        \"emocion\": emocion,\n",
        "        \"comprension\": comprension,\n",
        "        \"ticket\": ticket,\n",
        "        \"transcripcion\": transcripcion\n",
        "    }\n",
        "    global registros\n",
        "    registros.append(registro)\n",
        "    return f\"[LOG] Conversación guardada (ID: {len(registros)}). Fecha: {fecha}\"\n",
        "\n",
        "# === Backend: Función Principal ===\n",
        "def procesar_pregunta(pregunta: str):\n",
        "    \"\"\"Procesa la pregunta con RAG (semántico) y luego clasifica la transcripción (pregunta + respuesta).\"\"\"\n",
        "    if not pregunta.strip():\n",
        "        return \"Error: Ingresa una pregunta válida.\", \"\", \"\", \"\", \"\", \"[LOG] Error: Pregunta vacía.\"\n",
        "\n",
        "    log = \"[LOG] Iniciando procesamiento de pregunta...\\n\"\n",
        "\n",
        "    # Paso 1: Búsqueda semántica en Azure Search (Retrieval)\n",
        "    contexto = search_documents(pregunta)\n",
        "    log += f\"[LOG] Paso 1: Búsqueda semántica completada. Contexto recuperado: {len(contexto)} caracteres.\\n\"\n",
        "    if \"No se encontraron\" in contexto:\n",
        "        log += \"[LOG] ADVERTENCIA: Búsqueda sin resultados. Verifica el índice en Azure Portal (poblado con chunks de documentos vocacionales).\\n\"\n",
        "\n",
        "    # Paso 2: Generar respuesta con RAG (Generation)\n",
        "    respuesta = generate_answer(pregunta, contexto)\n",
        "    log += \"[LOG] Paso 2: Respuesta generada vía Azure OpenAI.\\n\"\n",
        "\n",
        "    # Crear transcripción para clasificación: pregunta + respuesta\n",
        "    transcripcion = f\"Pregunta del usuario: {pregunta}\\nRespuesta del mentor: {respuesta}\"\n",
        "\n",
        "    # Paso 3: Clasificar tema\n",
        "    tema = clasificar_tema(transcripcion)\n",
        "    log += f\"[LOG] Paso 3: Tema clasificado como '{tema}' vía Azure OpenAI.\\n\"\n",
        "\n",
        "    # Paso 4: Detectar emoción\n",
        "    emocion = detectar_emocion(transcripcion)\n",
        "    log += f\"[LOG] Paso 4: Emoción detectada como '{emocion}' vía Azure OpenAI.\\n\"\n",
        "\n",
        "    # Paso 5: Estimar comprensión\n",
        "    comprension = estimar_comprension(transcripcion)\n",
        "    log += f\"[LOG] Paso 5: Nivel de comprensión estimado en '{comprension}' vía Azure OpenAI.\\n\"\n",
        "\n",
        "    # Paso 6: Generar ticket de salida\n",
        "    ticket = generar_ticket_salida(transcripcion, tema)\n",
        "    log += f\"[LOG] Paso 6: Ticket de salida generado para '{tema}'.\\n\"\n",
        "\n",
        "    # Paso 7: Guardar conversación\n",
        "    save_log = guardar_conversacion(transcripcion, tema, emocion, comprension, ticket)\n",
        "    log += save_log + \"\\n\"\n",
        "\n",
        "    log += \"[LOG] Procesamiento completado exitosamente con Azure (búsqueda semántica activada).\"\n",
        "\n",
        "    return respuesta, tema, emocion, comprension, ticket, log\n",
        "\n",
        "# === Inicialización del Registro ===\n",
        "registros = []\n",
        "\n",
        "# === Frontend: Interfaz Gradio ===\n",
        "iface = gr.Interface(\n",
        "    fn=procesar_pregunta,\n",
        "    inputs=gr.Textbox(lines=5, label=\"Ingresa tu pregunta sobre orientación vocacional\", placeholder=\"Ejemplo: ¿Qué debo hacer si perdí mis beneficios del crédito universitario?\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(lines=10, label=\"Respuesta del RAG (Búsqueda Semántica + OpenAI)\"),\n",
        "        gr.Textbox(label=\"Tema identificado (Clasificación)\"),\n",
        "        gr.Textbox(label=\"Emoción detectada (Clasificación)\"),\n",
        "        gr.Textbox(label=\"Nivel de comprensión (Clasificación)\"),\n",
        "        gr.Textbox(lines=10, label=\"Ticket de salida generado (Clasificación)\"),\n",
        "        gr.Textbox(lines=7, label=\"Logs del sistema (Backend con Azure - Incluye Debug)\")\n",
        "    ],\n",
        "    title=\"Chat Vocacional con RAG Semántico y Clasificación (Azure)\",\n",
        "    description=\"Ingresa una pregunta para obtener una respuesta mejorada del sistema RAG (búsqueda semántica en Azure Search + generación con Azure OpenAI). Luego, clasifica la conversación. Logs incluyen debug para depurar búsquedas vacías.\"\n",
        ")\n",
        "\n",
        "# Lanzar la app en Colab\n",
        "iface.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "-zuQGMbN28Nx",
        "outputId": "b4b5ba9a-561b-4e80-cf14-eda53bc7cf41"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4fb34f0c7bcaebeba3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4fb34f0c7bcaebeba3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ERROR] Error HTTP en búsqueda: 400 - {\"error\":{\"code\":\"\",\"message\":\"Invalid expression: Could not find a property named 'content' on type 'search.document'.\\r\\nParameter name: $select\"}}\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://4fb34f0c7bcaebeba3.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import AzureOpenAI\n",
        "import requests\n",
        "import datetime\n",
        "import json\n",
        "from functools import lru_cache  # Para caché simple\n",
        "\n",
        "# === Configuración de Azure (Search y OpenAI) ===\n",
        "load_dotenv()  # Carga variables de .env\n",
        "\n",
        "# Validar variables requeridas\n",
        "required_vars = [\n",
        "    \"AZURE_SEARCH_API_KEY\", \"AZURE_SEARCH_ENDPOINT\", \"AZURE_SEARCH_INDEX_NAME\",\n",
        "    \"AZURE_OPENAI_API_KEY\", \"AZURE_OPENAI_ENDPOINT\", \"AZURE_OPENAI_DEPLOYMENT_NAME\"\n",
        "]\n",
        "for var in required_vars:\n",
        "    if not os.environ.get(var):\n",
        "        raise ValueError(f\"Falta variable de entorno: {var}\")\n",
        "\n",
        "# Configuración de Azure Search\n",
        "AZURE_SEARCH_API_KEY = os.environ[\"AZURE_SEARCH_API_KEY\"]\n",
        "AZURE_SEARCH_ENDPOINT = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
        "AZURE_SEARCH_INDEX_NAME = os.environ[\"AZURE_SEARCH_INDEX_NAME\"]\n",
        "\n",
        "# Inicializar cliente de Azure OpenAI\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    api_version=\"2023-08-01-preview\",\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
        ")\n",
        "\n",
        "# === Función para Obtener Esquema del Índice (Nueva para Depuración) ===\n",
        "def get_index_schema():\n",
        "    \"\"\"Obtiene los campos disponibles en el índice de Azure Search.\"\"\"\n",
        "    try:\n",
        "        url = f\"{AZURE_SEARCH_ENDPOINT}/indexes/{AZURE_SEARCH_INDEX_NAME}?api-version=2023-07-01-Preview\"\n",
        "        headers = {\"api-key\": AZURE_SEARCH_API_KEY}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        schema = response.json()\n",
        "        fields = schema.get(\"fields\", [])\n",
        "        searchable_fields = [field[\"name\"] for field in fields if field.get(\"searchable\", False)]\n",
        "        print(f\"[DEBUG] Campos disponibles en el índice (buscables): {searchable_fields}\")\n",
        "        return searchable_fields\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] No se pudo obtener el esquema: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Obtener esquema al inicio para depuración\n",
        "available_fields = get_index_schema()\n",
        "\n",
        "# Campo por defecto para contenido; ajusta si tu índice usa otro (ej. 'text' o 'description')\n",
        "DEFAULT_CONTENT_FIELD = \"content\"  # Cambia esto si es necesario, basado en available_fields\n",
        "\n",
        "# === Función de Búsqueda en Azure Search (Mejorada con Semántica y Manejo de Errores) ===\n",
        "@lru_cache(maxsize=100)  # Caché simple para evitar búsquedas repetidas\n",
        "def search_documents(query: str, top_k: int = 5) -> str:\n",
        "    \"\"\"Realiza una búsqueda semántica en Azure Cognitive Search y retorna el contexto concatenado.\"\"\"\n",
        "    try:\n",
        "        # Usar campos válidos dinámicamente\n",
        "        content_field = next((f for f in [DEFAULT_CONTENT_FIELD, \"text\", \"description\"] if f in available_fields), None)\n",
        "        if not content_field:\n",
        "            return \"Error: No se encontró un campo de contenido válido en el índice. Revisa available_fields en logs y ajusta DEFAULT_CONTENT_FIELD.\"\n",
        "\n",
        "        select_fields = [content_field, \"source\", \"@search.score\"]\n",
        "        url = f\"{AZURE_SEARCH_ENDPOINT}/indexes/{AZURE_SEARCH_INDEX_NAME}/docs/search?api-version=2023-07-01-Preview\"\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"api-key\": AZURE_SEARCH_API_KEY\n",
        "        }\n",
        "        payload = {\n",
        "            \"search\": query,\n",
        "            \"top\": top_k,\n",
        "            \"queryType\": \"semantic\",\n",
        "            \"semanticConfiguration\": \"default\",  # Ajusta si usas una config personalizada\n",
        "            \"select\": \",\".join(select_fields),\n",
        "            \"filter\": \"category eq 'beneficios_estudiantiles'\"  # Filtro opcional; ajusta o elimina si no aplica\n",
        "        }\n",
        "        print(f\"[DEBUG] Payload enviado a Azure Search: {json.dumps(payload, indent=2)[:300]}...\")  # Log parcial para evitar spam\n",
        "\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        results = response.json()\n",
        "\n",
        "        print(f\"[DEBUG] Respuesta de Azure Search (resumen): {len(results.get('value', []))} documentos encontrados.\")\n",
        "\n",
        "        documents = [doc.get(content_field, \"\") for doc in results.get(\"value\", [])]\n",
        "        contexto = \"\\n\\n\".join(documents)\n",
        "        if not contexto.strip():\n",
        "            return \"No se encontraron documentos relevantes para esta consulta. Verifica el índice o reformula la pregunta.\"\n",
        "        return contexto\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        error_msg = f\"Error HTTP en búsqueda: {e.response.status_code} - {e.response.text[:200]}\"\n",
        "        print(f\"[ERROR] {error_msg}\")\n",
        "        return f\"Error en búsqueda: {error_msg}\"\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error inesperado en búsqueda: {str(e)}\"\n",
        "        print(f\"[ERROR] {error_msg}\")\n",
        "        return f\"Error en búsqueda: {error_msg}\"\n",
        "\n",
        "# === Función de Generación de Respuesta (Actualizada con Mensaje de Fallback) ===\n",
        "def generate_answer(question: str, context: str) -> str:\n",
        "    \"\"\"Genera una respuesta usando Azure OpenAI basada en pregunta y contexto de búsqueda.\"\"\"\n",
        "    if not context or \"No se encontraron\" in context or len(context) < 100:  # Umbral para contexto insuficiente\n",
        "        return \"No pudimos recuperar esa respuesta. Escribe a contacto@innovacien.org si sigues teniendo dudas.\"\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    Eres un mentor pedagógico especializado en orientación vocacional y beneficios estudiantiles en Chile (becas, créditos, subvenciones del Mineduc, etc.).\n",
        "    Responde de forma clara, breve, útil y empática, basándote únicamente en el contexto proporcionado.\n",
        "    Estructura la respuesta con pasos accionables si aplica (ej. contactar Mineduc, revisar DEMRE).\n",
        "    Si el contexto es insuficiente o no contiene información relevante, responde: \"No pudimos recuperar esa respuesta. Escribe a contacto@innovacien.org si sigues teniendo dudas.\"\n",
        "    \"\"\"\n",
        "    user_prompt = f\"Pregunta: {question}\\n\\nContexto:\\n{context[:1500]}\"  # Limitar para tokens\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            max_tokens=500\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error al generar respuesta: {str(e)}\"\n",
        "\n",
        "# === Funciones de Clasificación (Sin Cambios, pero con límite de tokens) ===\n",
        "def clasificar_tema(transcripcion: str) -> str:\n",
        "    \"\"\"Clasifica el tema usando Azure OpenAI.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Clasifica el tema principal de esta transcripción de una sesión de mentoría en una de estas categorías:\n",
        "    Biología, Matemáticas, Lenguaje, Historia, Orientación Vocacional, Educación Superior, Beneficios Estudiantiles, Otros.\n",
        "    Responde solo con el nombre de la categoría.\n",
        "\n",
        "    Transcripción: {transcripcion[:1000]}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def detectar_emocion(transcripcion: str) -> str:\n",
        "    \"\"\"Detecta la emoción usando Azure OpenAI.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Detecta la emoción predominante en esta transcripción de mentoría (Entusiasmo, Confusión, Curiosidad, Frustración, Motivación, Ansiedad).\n",
        "    Responde solo con el nombre de la emoción.\n",
        "\n",
        "    Transcripción: {transcripcion[:1000]}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def estimar_comprension(transcripcion: str) -> str:\n",
        "    \"\"\"Estima el nivel de comprensión usando Azure OpenAI.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Estima el nivel de comprensión del estudiante en esta transcripción (Alto (80-100%), Medio (60-79%), Bajo (<60%)).\n",
        "    Responde solo con el nivel.\n",
        "\n",
        "    Transcripción: {transcripcion[:1000]}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def generar_ticket_salida(transcripcion: str, tema: str) -> str:\n",
        "    \"\"\"Genera un ticket de salida usando Azure OpenAI, adaptado al tema.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Genera 3 preguntas de ticket de salida relevantes para reforzar el aprendizaje en una sesión de mentoría sobre '{tema}' en contexto chileno (e.g., Mineduc, crédito universitario).\n",
        "    Incluye una mezcla de preguntas de opción múltiple y abiertas.\n",
        "    Formato: Pregunta 1\\n[opciones si aplica]\\n\\nPregunta 2\\n...\\n\\nPregunta 3\\n...\n",
        "\n",
        "    Basado en esta transcripción: {transcripcion[:1000]}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def guardar_conversacion(transcripcion: str, tema: str, emocion: str, comprension: str, ticket: str) -> str:\n",
        "    \"\"\"Guarda en registro (simulado) y retorna log.\"\"\"\n",
        "    fecha = datetime.datetime.now().isoformat()\n",
        "    registro = {\n",
        "        \"fecha\": fecha,\n",
        "        \"tema\": tema,\n",
        "        \"emocion\": emocion,\n",
        "        \"comprension\": comprension,\n",
        "        \"ticket\": ticket,\n",
        "        \"transcripcion\": transcripcion\n",
        "    }\n",
        "    global registros\n",
        "    registros.append(registro)\n",
        "    return f\"[LOG] Conversación guardada (ID: {len(registros)}). Fecha: {fecha}\"\n",
        "\n",
        "# === Backend: Función Principal (Mejorada con Validación) ===\n",
        "def procesar_pregunta(pregunta: str):\n",
        "    \"\"\"Procesa la pregunta con RAG (semántico) y luego clasifica la transcripción (pregunta + respuesta).\"\"\"\n",
        "    if not pregunta.strip():\n",
        "        return \"Error: Ingresa una pregunta válida.\", \"\", \"\", \"\", \"\", \"[LOG] Error: Pregunta vacía.\"\n",
        "\n",
        "    log = \"[LOG] Iniciando procesamiento de pregunta...\\n\"\n",
        "\n",
        "    # Validación simple para relevancia\n",
        "    if any(palabra in pregunta.lower() for palabra in [\"beneficios\", \"estudiantiles\", \"beca\", \"crédito\"]):\n",
        "        log += \"[LOG] Pregunta relacionada con beneficios estudiantiles detectada.\\n\"\n",
        "    else:\n",
        "        log += \"[ADVERTENCIA] La pregunta podría no estar enfocada en beneficios estudiantiles.\\n\"\n",
        "\n",
        "    # Paso 1: Búsqueda semántica en Azure Search (Retrieval)\n",
        "    contexto = search_documents(pregunta)\n",
        "    log += f\"[LOG] Paso 1: Búsqueda semántica completada. Contexto recuperado: {len(contexto)} caracteres.\\n\"\n",
        "    if \"No se encontraron\" in contexto or \"Error\" in contexto:\n",
        "        log += \"[LOG] ADVERTENCIA: Búsqueda sin resultados o con error. Usando fallback en generación.\\n\"\n",
        "\n",
        "    # Paso 2: Generar respuesta con RAG (Generation)\n",
        "    respuesta = generate_answer(pregunta, contexto)\n",
        "    log += \"[LOG] Paso 2: Respuesta generada vía Azure OpenAI.\\n\"\n",
        "\n",
        "    # Crear transcripción para clasificación: pregunta + respuesta\n",
        "    transcripcion = f\"Pregunta del usuario: {pregunta}\\nRespuesta del mentor: {respuesta}\"\n",
        "\n",
        "    # Paso 3: Clasificar tema\n",
        "    tema = clasificar_tema(transcripcion)\n",
        "    log += f\"[LOG] Paso 3: Tema clasificado como '{tema}' vía Azure OpenAI.\\n\"\n",
        "\n",
        "    # Paso 4: Detectar emoción\n",
        "    emocion = detectar_emocion(transcripcion)\n",
        "    log += f\"[LOG] Paso 4: Emoción detectada como '{emocion}' vía Azure OpenAI.\\n\"\n",
        "\n",
        "    # Paso 5: Estimar comprensión\n",
        "    comprension = estimar_comprension(transcripcion)\n",
        "    log += f\"[LOG] Paso 5: Nivel de comprensión estimado en '{comprension}' vía Azure OpenAI.\\n\"\n",
        "\n",
        "    # Paso 6: Generar ticket de salida\n",
        "    ticket = generar_ticket_salida(transcripcion, tema)\n",
        "    log += f\"[LOG] Paso 6: Ticket de salida generado para '{tema}'.\\n\"\n",
        "\n",
        "    # Paso 7: Guardar conversación\n",
        "    save_log = guardar_conversacion(transcripcion, tema, emocion, comprension, ticket)\n",
        "    log += save_log + \"\\n\"\n",
        "\n",
        "    log += \"[LOG] Procesamiento completado exitosamente con Azure (búsqueda semántica activada).\"\n",
        "\n",
        "    return respuesta, tema, emocion, comprension, ticket, log\n",
        "\n",
        "# === Inicialización del Registro ===\n",
        "registros = []\n",
        "\n",
        "# === Frontend: Interfaz Gradio ===\n",
        "iface = gr.Interface(\n",
        "    fn=procesar_pregunta,\n",
        "    inputs=gr.Textbox(lines=5, label=\"Ingresa tu pregunta sobre orientación vocacional o beneficios estudiantiles\", placeholder=\"Ejemplo: ¿Qué debo hacer si perdí mis beneficios del crédito universitario?\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(lines=10, label=\"Respuesta del RAG (Búsqueda Semántica + OpenAI)\"),\n",
        "        gr.Textbox(label=\"Tema identificado (Clasificación)\"),\n",
        "        gr.Textbox(label=\"Emoción detectada (Clasificación)\"),\n",
        "        gr.Textbox(label=\"Nivel de comprensión (Clasificación)\"),\n",
        "        gr.Textbox(lines=10, label=\"Ticket de salida generado (Clasificación)\"),\n",
        "        gr.Textbox(lines=7, label=\"Logs del sistema (Backend con Azure - Incluye Debug)\")\n",
        "    ],\n",
        "    title=\"Chat Vocacional con RAG Semántico y Clasificación (Azure) - Versión Mejorada\",\n",
        "    description=\"Ingresa una pregunta para obtener una respuesta mejorada. Si no hay datos suficientes, se sugiere contactar a contacto@innovacien.org. Logs incluyen esquema del índice para depuración.\"\n",
        ")\n",
        "\n",
        "# Lanzar la app\n",
        "iface.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "agPaVzUQAYVv",
        "outputId": "21d7baa5-1f6f-42b3-a5fa-bfbb75f2276e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ERROR] No se pudo obtener el esquema: 403 Client Error: Forbidden for url: https://search-vocatest.search.windows.net/indexes/rag-vocacional?api-version=2023-07-01-Preview\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://80bb2f24f801537d72.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://80bb2f24f801537d72.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://80bb2f24f801537d72.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}